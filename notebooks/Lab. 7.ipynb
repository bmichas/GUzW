{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie standardowych bibliotek"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie bibliotek do obsługi sieci neuronowych"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Input, Activation\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.math import log\n","from tensorflow.math import reduce_sum\n","from tensorflow.python.framework.ops import disable_eager_execution\n","disable_eager_execution()"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","<p style='text-align: justify;'>\n","Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n","    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n","    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n","Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n","</p>"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["class Agent:\n","    def __init__(self, state_size, action_size, actor, critic, policy_model):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.action_space = [i for i in range(action_size)]\n","        self.gamma = 0.99    # discount rate\n","        self.learning_rate = 0.001\n","        self.policy_model = policy_model\n","        self.actor = actor\n","        self.critic = critic #critic network should have only one output\n","\n","\n","    def choose_action(self, state):\n","        \"\"\"\n","        Compute the action to take in the current state, basing on policy returned by the network.\n","\n","        Note: To pick action according to the probability generated by the network\n","        \"\"\"\n","\n","        #\n","        # INSERT CODE HERE to get action in a given state\n","        #\n","        state = state[np.newaxis, :]\n","        probabilities = self.policy_model.predict(state, verbose = 0)[0]\n","        chosen_action = np.random.choice(self.action_space, p=probabilities)        \n","        \n","        return chosen_action\n","\n","  \n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for state and next_state should be estimated based on output of critic network.\n","        Critic network should be trained based on target value:\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","\n","        state = state[np.newaxis, :]\n","        next_state = next_state[np.newaxis, :]\n","        critic_value_next_state = self.critic.predict(next_state)\n","        critic_value = self.critic.predict(state)\n","\n","        target = reward + self.gamma * critic_value_next_state * (1 - int(done))\n","        delta = target - critic_value\n","\n","        actions = np.zeros([1, self.action_size])\n","        actions[np.arange(1), action] = 1\n","\n","        self.actor.fit([state, delta], actions, verbose=0)\n","        self.critic.fit(state, target, verbose=0)  \n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bartycja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["env = gym.make(\"CartPole-v1\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","alpha_learning_rate = 0.0001\n","beta_learning_rate = 0.0005\n","\n","input = Input(shape=(state_size,))\n","delta = Input(shape=[1])\n","x1 = Dense(64, activation='relu')(input)\n","probs = Dense(action_size, activation='softmax')(x1)\n","values = Dense(1, activation='linear')(x1)\n","\n","policy_model = Model(inputs=[input], outputs=[probs])\n","\n","def custom_loss(y_true, y_pred):\n","    log_lik = y_true * log(y_pred)\n","    return reduce_sum(-log_lik * delta)\n","\n","#\n","# INSERT CODE HERE to build actor network, in the last layer use softmax activation function\n","#\n","\n","actor_model = Model(inputs=[input, delta], outputs=[probs])\n","actor_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=alpha_learning_rate), loss=custom_loss)\n","       \n","#\n","# INSERT CODE HERE to build critic network, in the last layer use linear activation function, network should have single output\n","#\n","\n","critic_model = Model(inputs=[input], outputs=[values])\n","critic_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=beta_learning_rate), loss='mean_squared_error')"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyć agenta gry w środowisku *CartPool*:"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bartycja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"name":"stdout","output_type":"stream","text":["mean reward:13.560\n","mean reward:15.810\n","mean reward:21.780\n","mean reward:36.820\n","mean reward:49.920\n","mean reward:87.760\n","mean reward:122.420\n","mean reward:130.330\n","mean reward:142.780\n","mean reward:126.610\n","mean reward:99.810\n","mean reward:147.430\n","mean reward:139.830\n","mean reward:123.310\n","mean reward:172.640\n","mean reward:290.270\n","mean reward:168.730\n","mean reward:204.850\n","mean reward:2027.140\n","You Win!\n"]}],"source":["agent = Agent(state_size, action_size, actor_model, critic_model, policy_model)\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()[0]\n","        while not done:\n","            action = agent.choose_action(state)\n","            next_state, reward, done, info, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
