{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9wlx8wSpIi9"
      },
      "source": [
        "# Deep Q-Network implementation\n",
        "\n",
        "This notebook shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way.\n",
        "\n",
        "Based on Yandex School of Data Analysis Practical RL course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "i8q_pgOXpIi_"
      },
      "outputs": [],
      "source": [
        "#XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "HFCWxsAkpIjA"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9eKYAoIpIjA"
      },
      "source": [
        "### Let's play some old videogames\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/nerd.png)\n",
        "\n",
        "This time we're gonna apply approximate q-learning to an atari game called Breakout. It's not the hardest thing out there, but it's definitely way more complex than anything we tried before.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJhThPOKpIjA"
      },
      "source": [
        "### Processing game image \n",
        "\n",
        "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
        "\n",
        "We can thus save a lot of time by preprocessing game image, including\n",
        "* Resizing to a smaller shape, 64 x 64\n",
        "* Converting to grayscale\n",
        "* Cropping irrelevant image parts (top & bottom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sdpPAb2ByhFf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from gymnasium.core import ObservationWrapper\n",
        "from gymnasium.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PreprocessAtari(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
        "        ObservationWrapper.__init__(self, env)\n",
        "\n",
        "        self.img_size = (64, 64)\n",
        "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
        "\n",
        "    def observation(self, img):\n",
        "        \"\"\"what happens to each observation\"\"\"\n",
        "\n",
        "        # Here's what you need to do:\n",
        "        #  * Crop image, remove irrelevant parts.\n",
        "        #  * Resize image to self.img_size. Use cv2.resize or any other library you want,\n",
        "        #    e.g. PIL or Keras. Do not use skimage.transform.resize because it is roughly\n",
        "        #    6x slower than cv2.resize.\n",
        "        #  * Cast image to grayscale.\n",
        "        #  * Convert image pixels to (0, 1) range, float32 type.\n",
        "\n",
        "        img = img[94:-5,7:-7]\n",
        "        img = cv2.resize(img, self.img_size)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        img = np.expand_dims(img,-1)\n",
        "        img_array = np.array(img)\n",
        "        img_array = img_array.astype('float32') / 255\n",
        "        # print(img_array.shape)\n",
        "        return img_array\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1c33757ea30>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"BreakoutDeterministic-v4\")\n",
        "env = PreprocessAtari(env)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "plt.title(\"Agent observation\")\n",
        "plt.imshow(obs, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgvevvBIpIjB"
      },
      "source": [
        "### Frame buffer\n",
        "\n",
        "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to fing optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n",
        "\n",
        "To do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "za3pVaaWqnwX"
      },
      "outputs": [],
      "source": [
        "from gymnasium.spaces.box import Box\n",
        "from gymnasium.core import Wrapper\n",
        "class FrameBuffer(Wrapper):\n",
        "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        self.dim_order = dim_order\n",
        "        if dim_order == 'tensorflow':\n",
        "            height, width, n_channels = env.observation_space.shape\n",
        "            obs_shape = [height, width, n_channels * n_frames]\n",
        "        elif dim_order == 'pytorch':\n",
        "            n_channels, height, width = env.observation_space.shape\n",
        "            obs_shape = [n_channels * n_frames, height, width]\n",
        "        else:\n",
        "            raise ValueError('dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset()[0])\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info, _ = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info\n",
        "    \n",
        "    def update_buffer(self, img):\n",
        "        if self.dim_order == 'tensorflow':\n",
        "            offset = self.env.observation_space.shape[-1]\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        elif self.dim_order == 'pytorch':\n",
        "            offset = self.env.observation_space.shape[0]\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "id": "wWEBJq27pIjC",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
        "    env = PreprocessAtari(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "id": "LpGgZ-5BpIjC"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAACDCAYAAAB/X/s/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU00lEQVR4nO3de7hVdZ3H8feHA0dAUBEMQTRvWGkzqUPilEw9XbTsgvWUaaan0pwsHXJyTKtpnKbmqWYy9alpsqyoTO0e45RpKFFNXrDMFDTxFhBwBFFBQeDwnT9+v6Ob3b5xzr5w1vm8nmc/Z631W5fv+q3f+u7fXmvtfRQRmJnZ0Dai0wGYmdngOZmbmRWAk7mZWQE4mZuZFYCTuZlZATiZm5kVgJN5AUnaX1JIGtnpWHaEpFMkXd+ide8iabGkKQNc/ixJqyVtkDSx2fG1wo62A0mfkLRG0qpWx1YjhrslvbzBeR+S9KodWPc5kj490Nh2dk7mDZC0QNI6Sbu0cZsh6eB2ba/dKiWaiLgyIo5t0SbPBBZGxMqyOLolLZG0vEaso4CLgWMjYlxErG1RjB0jaT/gg8ChEbG3pHdK+lWdZRZIOqOZcUTEYRGxYLDrkfTyCsf0y8Apkp4z2PXvjJzM65C0PzALCOCNnY1m56FkKLWf9wLfrDD9n4BH6iw7GRgN3F2pcKh9AqpiP2BtRPR2YuPtqMOI2AT8FDit1dvqiIjwq8YL+Bjwa1LP7NqysonA/wBPALcBnwB+VVL+fOAG4FHgXuDEkrKvA18A/hdYD9wCHJTLFpLePJ4ENgBvqxDXCOCjwMNAL/ANYPdctn9e/kzgz8BK4LySZY8CFuW4VwMXl5QdDfwf8Bjwe+DlJWULgE/m+tgIfAhYVBbXucC8PPw64Hd5O8uAi0rm+1OOcUN+/S3wzrL6e0mu18fz35eUxfJvOZb1wPXApCrHcL8c78iy6QcAS4DXAsurLHtIPg79sd6YpwfwfuA+4ME87dK8n08AtwOzStZzEfBd4Fs53j/kdV+Yj98yUs+/f/7dgSvysVtBaltduexg4Be5XtYA11SJvb8djKy1TuBVuX625X28BtgE9OXxxyqs+5O5fFOe5/P1jlmFdTyU29CdwNPAyDztVbl8DDAXWJeP0/mlxynPe15e/vEc92hg17L92QBMzcucAtzU6bzSklzV6QB29hewFHgf8DfAFmBySdnV+TUWODSfkL/KZbvm8XflRnpEPvEOzeVfB9aSEutI4Erg6pJ1B3BwjbjenWM7EBgH/AD4Zi7rP4mvynH8Fan32X+S/AY4NQ+PA47Ow/vkmI4nvVm8Oo/vlcsXkJLwYTnm3UmJaXpJXLcBJ+Xhl+dtjwD+mvTGcUJZjCNLln1nSf3tmU/iU/O2Ts7jE0tiuZ+UEMfk8U9VqavXAXdXmH4t8KYcZ8VkXiPWIL1R7wmMydPeQXqDH0m6ZLEKGJ3LLiIlvuNy+TeAB4GPAKOA95DfFPL8PwS+lI/fc4Bbgb/PZVfl5UaQktcxjcRdZ53b1QFlb6xV1r8AOKNkvOYxq7D8Q8AdwL4ldfgQz7bTT5HetCYA00hJuzyZ3wpMzdteAry30v6ULHMk8Gin80orXh0PYGd+AceQEvikPH4PcG4e7splzyuZ/5meOfA24Jdl6/sS8C95+OvAV0rKjgfuKRmvl8znA+8rGX9ejmdkyUn8/JLyzwBX5OGFwL9S1pMl9ZK+WTbtZ0BPHl4AfLys/FvAx/LwdFJyH1sl5kuAz+Xh/hirJfNTgVvLlv8N8M6SWD5aUvY+4Loq2z0FuLls2puAn+bhiid+ybyVYg3gFXXazzrgRXn4IuCGkrI3kHqM/b3t8Xmde5Au6zxNTnC5/GRyj5L0RnA5MK3O9p+Ju4F1blcHDCyZ1zxmFZZ/CHh3hWn9yfwB4LiSsjP4y2T+jrI2/t+1jmluo3219muovobSNc9O6AGuj4g1efzbeRrAXqSTZFnJ/KXDzwVmSnqs/0VKKnuXzFP61MBTpF5yo6aSLrH0e5hnT9pK8TyclwE4ndSjvUfSbZJeXxLzW8tiPgYofQKkdJ2Q6uTkPPx24EcR8RSApJmSbpL0iKTHSdetJw1w//r3YZ+S8Ubrbx0pWZLj2pV04v9Dg7FUs11dSDov30x9PNfd7my/v6tLhjcCayKir2Qc0j48l9RbX1lyHL5E6k1Dutwg4Nb89Me7G4i13jqboZFjVq68PZWvr9r51W9Hz6HxpEsyhVOEGzctIWkMcCLQVfKo1i7AHpJeBNwFbCV9/PtjLt+3ZBXLgF9ExKtbFOKfSSdov/1yPKtzTP3x3FNS/meAiLgPODnfwHwz8L38uN0yUs/8PTW2G2XjNwB7STqclNTPLSn7NvB54LURsUnSJTyb3MrXU2//+vfhujrLVXIncICkkRGxldQ72x/4pSSAbmD3fJyPjoiHGlzvM/sgaRYpyb6SdElnm6R1pKS7o5aRetGTcrzbbzRiFemyDJKOAX4uaWFELB3oOiuod3wqzTOQY1ZrOytJbXlxHt+3xryNrvcFpHtBheOeeXUnkG7wHAocnl8vAH4JnJZ7VD8ALpI0VtLz2f4u+bXAIZJOlTQqv14s6QUNbn816Xp4NVcB50o6QNI44N9JN8JKT9R/zrEdRrp2fw2ApHdI2isitpFudEK6WfQt4A2SjpPUJWl0fsRrGlVExBbSjb3/IF23vKGkeDzp+uQmSUeReu79HsnbrLaPPyHV39sljZT0NtKxuLZGnVSLcTnp/sJRedJdpMRweH6dQarvw6ndU6xlPOnN9BFgpKSPAbsNZEWRHp+8HvispN0kjZB0kKSXAUh6a8kxWUdKXNsGs84KVgPTJHXXWG15G23aMcu+A1woaYKkfYCzd2DZ1cBESbuXTX8Z6YmWwnEyr64H+FpE/CkiVvW/SD3NU/KjVGeTPkqvIj32dhWp90NErAeOBU4i9VhWAZ8m9e4bcREwN38kPrFC+VfzNheSbqRtAs4pm+cXpCQ2H/jPiOj/Qs5rgLslbSA9gXFSRGyMiGXAbODDpKS0jPToXr128m3SExHfLXszeR/wcUnrSU8Ffae/IF+K+STw67yPR5euMNKz3K8n3UhcS+r1vr7kkteO+hLpmi4RsbXsmD4KbMvjfTXXUt3PSD3QP5IuLWxi4G8MkDoG3aRe6Trgezx7uevFwC35+M0D5kTEA4NcZ7kbSY9irpJUrc4vBd6Sv4NxWQuO2ceB5aT2/fMc79ONLBgR95DOxwdy+5oqaTTp3tTcAcazU1O+KWBNkL9dtndE9NSd2doqf+Hrd8Aro+yLQzY0SDqL1PGo9mmi3vLnAPtGxPnNjWzn4GQ+CPnSSjfpmeEXkz5mnhERP+pkXGZFoPTTCweSnoiZTvpOxucj4pJOxrWz8g3QwRlP+ig3lXSN7rPAjzsakVlxdJMujx1AurdzNfBfnQxoZzaonrmk15Cum3WRnpn+VLMCMzOzxg04mUvqIt3seTXpJsVtwMkRsbjmgmZm1nSDeZrlKGBpRDwQEZtJH4FmNycsMzPbEYO5Zr4P2z96tRyYWWuBsWPHRl9fH5s3bx7EZm38+PGMHTsWgN7eXnwTe3AmTZpEV1cXW7duZe3awv26bVtJ4jnPSV8qfeqpp1i/fn2HIxrauru7mTBhAqtXr14TEXvVmrflz5lLOlPSIkmLRo0axdSpU+svZDXNnDmTnp4eTj31VLq7a32nwxpxwgkn0NPTwxvf6F84HqxddtmF0047jZ6eHmbMmNHpcIa8qVOn0tPTA3/5Mwl/YTDJfAXbf712Wp62nYi4PCJmRMSMMWPGDGJzZmZWzWCS+W3A9Px18m7SNx3nNScsMzPbEQO+Zh4RWyWdTfoacxfw1Yio+J9YzMystQb1paGI+AnpW49mZtZB/qEtM7MCcDI3MysAJ3MzswJwMjczKwAnczOzAnAyNzMrACdzM7MCcDI3MysAJ3MzswJwMjczKwAnczOzAnAyNzMrACdzM7MCcDI3MysAJ3MzswJwMjczKwAnczOzAqibzCXtK+kmSYsl3S1pTp6+p6QbJN2X/05ofbhmZlZJIz3zrcAHI+JQ4Gjg/ZIOBS4A5kfEdGB+Hjczsw6om8wjYmVE/DYPrweWAPsAs4G5eba5wAktitHMzOrYoWvmkvYHjgBuASZHxMpctAqY3NzQzMysUQ0nc0njgO8DH4iIJ0rLIiKAqLLcmZIWSVq0cePGQQVrZmaVNZTMJY0iJfIrI+IHefJqSVNy+RSgt9KyEXF5RMyIiBljxoxpRsxmZlamkadZBFwBLImIi0uK5gE9ebgH+HHzwzMzs0aMbGCelwKnAn+QdEee9mHgU8B3JJ0OPAyc2JIIzcysrrrJPCJ+BahK8SubG46ZmQ2EvwFqZlYATuZmZgXgZG5mVgBO5mZmBeBkbmZWAE7mZmYF4GRuZlYATuZmZgXgZG5mVgBO5mZmBeBkbmZWAE7mZmYF4GRuZlYATuZmZgXgZG5mVgBO5mZmBeBkbmZWAA0nc0ldkn4n6do8foCkWyQtlXSNpO7WhWlmZrXsSM98DrCkZPzTwOci4mBgHXB6MwMzM7PGNZTMJU0DXgd8JY8LeAXwvTzLXOCEFsRnZmYNaLRnfglwPrAtj08EHouIrXl8ObBPc0MzM7NG1U3mkl4P9EbE7QPZgKQzJS2StGjjxo0DWYWZmdUxsoF5Xgq8UdLxwGhgN+BSYA9JI3PvfBqwotLCEXE5cDnA3nvvHU2J2szMtlO3Zx4RF0bEtIjYHzgJuDEiTgFuAt6SZ+sBftyyKM3MrKbBPGf+IeAfJS0lXUO/ojkhmZnZjmrkMsszImIBsCAPPwAc1fyQzMxsR/kboGZmBeBkbmZWAE7mZmYF4GRuZlYATuZmZgXgZG5mVgBO5mZmBeBkbmZWAE7mZmYF4GRuZlYATuZmZgXgZG5mVgBO5mZmBbBDv5o43HR1ddHV1VVznm3btrF169aa8zRbRLBt27b6Mw4BkhgxYkRH67mvr4+IYvzfFEl0dXUxYkT1flp/++nr62v69vvXXe947mwkMWrUqLrzbd68uQ3RbK/Rc72tybxVDahVzjnnHM4666ya81x33XXMmTOnTRElTz75JOvWrWP06NFDPglNnz6dWbNmcf7551edp7e3l9tvv51zzz23Jfv72GOPMWrUKDZt2tT0dbfbzJkzefOb38zs2bOrzrNkyRJuvPFGLrvssqZvPyJYs2YNY8eOZcuWLU1ff6scccQRXHXVVTXn6e3tZdasWW2KKNmyZQtr1qxpaN6298yHUvKZOHEihxxySM157rzzzjZF86ytW7fS19dXiN55d3c3EyZMqFnP48aNY8WKFUhqSfvpr8uh1DarGTduHFOmTKlZn0899RQTJ05sWQxD8ZPOmDFj6p7ru+22W5uieVZENNwB9mWWGh555BEWL15cc57ly5e3KZpi2rRpE2vXrq1Zz729vSxfvnzIJYhO2LBhAytWrKhZn/fffz+9vb1tjGrn9+STT9Y91xvtIXdKQ8lc0h7AV4AXAgG8G7gXuAbYH3gIODEi1rUiyE657LLLWvJR1J61dOlSli5dyte+9rVOh1IIN998MzfffDMXXHBBx2IYim+6d9xxB4cddlinw6io0fps9GmWS4HrIuL5wIuAJcAFwPyImA7Mz+PWBnfddRcLFy7sdBiFMX/+fO65555Oh1EImzdvZt68eaxbV6h+Xcf09vYyb968xmaOiJovYHfgQUBl0+8FpuThKcC99dY1evToGDduXJB6934N8CUpdt1113jhC18YI0aM6Hg8Q/0lKSZPnhwHH3xwx2MpwktSHHTQQTFlypSOx1KEl6QAFtXLr6rXhZd0OHA5sJjUK78dmAOsiIg98jwC1vWP11hX7Y2ZmVklt0fEjFozNHKZZSRwJPDFiDgCeJKySyqR3hEqJmpJZ0paJGlRYzGbmdmOaiSZLweWR8Qtefx7pOS+WtIUgPy34u3xiLg8ImbUe1cxM7OBq5vMI2IVsEzS8/KkV5IuucwDevK0HuDHLYnQzMzqavQ583OAKyV1Aw8A7yK9EXxH0unAw8CJrQnRzMzqqXsDtKkb8w1QM7OBqHsDtN3fAN1AeqRxuJsE7NxfJ2s910HienAdQP06eG69FbQ7md/rG6EgadFwrwfXQeJ6cB1Ac+rAv2duZlYATuZmZgXQ7mR+eZu3t7NyPbgO+rkeXAfQhDpo69MsZmbWGr7MYmZWAG1L5pJeI+leSUslDZufy5X0kKQ/SLqj//dpJO0p6QZJ9+W/EzodZ7NJ+qqkXkl3lUyruN9KLstt405JR3Yu8uapUgcXSVqR28Mdko4vKbsw18G9ko7rTNTNJWlfSTdJWizpbklz8vTh1haq1UPz2kO9n1VsxgvoAu4HDgS6gd8Dh7Zj251+kf5xx6SyaZ8BLsjDFwCf7nScLdjvvyP9hs9d9fYbOB74KSDgaOCWTsffwjq4CDivwryH5vNiF+CAfL50dXofmlAHU4Aj8/B44I95X4dbW6hWD01rD+3qmR8FLI2IByJiM3A1MLtN294ZzQbm5uG5wAmdC6U1ImIh8GjZ5Gr7PRv4RiQ3A3v0/4jbUFalDqqZDVwdEU9HxIPAUtJ5M6RFxMqI+G0eXk/6xzb7MPzaQrV6qGaH20O7kvk+wLKS8eXU3pEiCeB6SbdLOjNPmxwRK/PwKmByZ0Jru2r7Pdzax9n5EsJXSy6xFb4OJO0PHAHcwjBuC2X1AE1qD74B2nrHRMSRwGuB90v6u9LCSJ+pht0jRcN1v4EvAgcBhwMrgc92NJo2kTQO+D7wgYh4orRsOLWFCvXQtPbQrmS+Ati3ZHxanlZ4EbEi/+0Ffkj6qNTQb8EXULX9HjbtIyJWR0RfRGwDvsyzH50LWweSRpES2JUR8YM8edi1hUr10Mz20K5kfhswXdIB+Wd0TyL9HnqhSdpV0vj+YeBY4C6G72/BV9vvecBp+UmGo4HHSz6CF0rZ9d83kdoDpDo4SdIukg4ApgO3tju+Zsv/UvIKYElEXFxSNKzaQrV6aGp7aOPd3ONJd3DvBz7S6bvLbdrnA0l3pH8P3N2/38BEYD5wH/BzYM9Ox9qCfb+K9LFxC+l63+nV9pv05MIXctv4AzCj0/G3sA6+mffxznzCTimZ/yO5Du4FXtvp+JtUB8eQLqHcCdyRX8cPw7ZQrR6a1h78DVAzswLwDVAzswJwMjczKwAnczOzAnAyNzMrACdzM7MCcDI3MysAJ3MzswJwMjczK4D/B9f1LAALT4GnAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "for _ in range(50):\n",
        "    obs, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "\n",
        "plt.title(\"Agent observation (4 frames left to right)\")\n",
        "plt.imshow(obs.transpose([0,2,1]).reshape([state_dim[0],-1]), cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDYxJ0PLpIjC"
      },
      "source": [
        "### Building a network\n",
        "\n",
        "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
        "\n",
        "You can build any architecture you want, but for reference, here's something that will more or less work:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32PlDCSbpIjC"
      },
      "source": [
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/dqn_arch.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "FzaLNEcspIjC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1769: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "nMTzKp9dpIjD"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Dense, InputLayer, Conv2D, Dense, Flatten\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "        with tf.variable_scope(name, reuse=reuse):\n",
        "            # input = InputLayer(input_shape = state_shape)\n",
        "            # conv = Conv2D(16, (3, 3), strides = 2, activation = 'relu')(input)\n",
        "            # conv = Conv2D(32, (3, 3), strides = 2, activation = 'relu')(conv)\n",
        "            # conv = Conv2D(64, (3, 3), strides = 2, activation = 'relu')(conv)\n",
        "            # flat = Flatten()(conv)\n",
        "            # den = Dense(256, activation = 'relu')(flat)\n",
        "            # output = Dense(n_actions)(den)\n",
        "            # self.network = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "            self.network = Sequential()\n",
        "            self.network.add(Conv2D(16, (3, 3), strides = 2, activation = 'relu', input_shape = state_shape))\n",
        "            self.network.add(Conv2D(32, (3, 3), strides = 2, activation = 'relu'))\n",
        "            self.network.add(Conv2D(64, (3, 3), strides = 2, activation = 'relu'))\n",
        "            self.network.add(Flatten())\n",
        "            self.network.add(Dense(256, activation = 'relu'))\n",
        "            self.network.add(Dense(n_actions))\n",
        "            \n",
        "            \n",
        "\n",
        "            # prepare a graph for agent step\n",
        "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
        "            \n",
        "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_symbolic_qvalues(self, state_t):\n",
        "        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n",
        "        \n",
        "        qvalues = self.network(state_t)\n",
        "\n",
        "        \n",
        "        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n",
        "            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n",
        "        assert int(qvalues.shape[1]) == n_actions\n",
        "        \n",
        "        return qvalues\n",
        "    \n",
        "    def get_qvalues(self, state_t):\n",
        "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
        "        sess = tf.get_default_session()\n",
        "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
        "    \n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "7JbUbRFgpIjD"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)\n",
        "sess.run(tf.global_variables_initializer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0_Ha6PpIjD"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "vEGIgty6pIjD"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "            reward += r\n",
        "            if done: break\n",
        "                \n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "TeIZ-tFXpIjE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(env, agent, n_games=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWH6apQipIjE"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here __to get 2 bonus points__.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNFeeb_KpIjE"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "J1c_S3yJ4iCE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(obses_t),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(obses_tp1),\n",
        "            np.array(dones)\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [\n",
        "            random.randint(0, len(self._storage) - 1)\n",
        "            for _ in range(batch_size)\n",
        "        ]\n",
        "        return self._encode_sample(idxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "pa2njiLmpIjE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset(), env.action_space.sample(), 1.0, env.reset(), done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "mgRDuSqfpIjE"
      },
      "outputs": [],
      "source": [
        "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
        "    Whenever game ends, add record with done=True and reset the game.\n",
        "    It is guaranteed that env has done=False when passed to this function.\n",
        "    \n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "    \n",
        "    :returns: return sum of rewards over time\n",
        "    \"\"\"\n",
        "    # initial state\n",
        "    state = env.framebuffer\n",
        "    \n",
        "    # Play the game for n_steps as per instructions above\n",
        "    # <YOUR CODE>\n",
        "    total_reward = 0\n",
        "    for _ in range(n_steps):\n",
        "        q_val = agent.get_qvalues([state])\n",
        "        action = agent.sample_actions(q_val)[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        exp_replay.add(state, action, reward, next_state, done)\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "\n",
        "    return total_reward\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "ANBBKyMMpIjF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "# testing your code. This may take a minute...\n",
        "exp_replay = ReplayBuffer(20000)\n",
        "\n",
        "play_and_record(agent, env, exp_replay, n_steps=10000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction. \n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 10000, \"play_and_record should have added exactly 10000 steps, \"\\\n",
        "                                 \"but instead added %i\"%len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n",
        "                                    \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\"%(np.mean(is_dones), len(exp_replay))\n",
        "    \n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_dim\n",
        "    assert act_batch.shape == (10,), \"actions batch should have shape (10,) but is instead %s\"%str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \"rewards batch should have shape (10,) but is instead %s\"%str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (10,), \"is_done batch should have shape (10,) but is instead %s\"%str(is_done_batch.shape)\n",
        "    assert [int(i) in (0,1) for i in is_dones], \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a <= n_actions for a in act_batch], \"actions should be within [0, n_actions]\"\n",
        "    \n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCNiL7bOpIjF"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "hkBms5_-pIjF"
      },
      "outputs": [],
      "source": [
        "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": true,
        "id": "b0pdUT8MpIjF"
      },
      "outputs": [],
      "source": [
        "def load_weigths_into_target_network(agent, target_network):\n",
        "    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n",
        "    assigns = []\n",
        "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
        "        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
        "    tf.get_default_session().run(assigns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "-FWdtws-pIjG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It works!\n"
          ]
        }
      ],
      "source": [
        "load_weigths_into_target_network(agent, target_network) \n",
        "\n",
        "# check that it works\n",
        "sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]);\n",
        "print(\"It works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbwyIQi-pIjG"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "ndyEzMb3pIjG"
      },
      "outputs": [],
      "source": [
        "# placeholders that will be fed with exp_replay.sample(batch_size)\n",
        "obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "actions_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "rewards_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "is_done_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "is_not_done = 1 - is_done_ph\n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJiCrl69pIjG"
      },
      "source": [
        "Take q-values for actions agent just took"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "tK5SjIQ2pIjG"
      },
      "outputs": [],
      "source": [
        "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
        "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkkpLgvYpIjH"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "KfU8N4L4pIjH"
      },
      "outputs": [],
      "source": [
        "# next_qvalues_target = ### YOUR CODE: compute q-values for NEXT states with target network\n",
        "# next_state_values_target = ### YOUR CODE: compute state values by taking max over next_qvalues_target for all actions\n",
        "# reference_qvalues = ### YOUR CODE: compute Q_reference(s,a) as per formula above\n",
        "next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph)\n",
        "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
        "reference_qvalues = rewards_ph + gamma*next_state_values_target*is_not_done\n",
        "\n",
        "# Define loss function for sgd.\n",
        "td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
        "td_loss = tf.reduce_mean(td_loss)\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(td_loss, var_list=agent.weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "esKoPH3mpIjH"
      },
      "outputs": [],
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": true,
        "id": "ANWFZjNdpIjH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splendid!\n"
          ]
        }
      ],
      "source": [
        "for chk_grad in tf.gradients(reference_qvalues, agent.weights):\n",
        "    error_msg = \"Reference q-values should have no gradient w.r.t. agent weights. Make sure you used target_network qvalues! \"\n",
        "    error_msg += \"If you know what you're doing, ignore this assert.\"\n",
        "    assert chk_grad is None or np.allclose(sess.run(chk_grad), sess.run(chk_grad * 0)), error_msg\n",
        "\n",
        "assert tf.gradients(reference_qvalues, is_not_done)[0] is not None, \"make sure you used is_not_done\"\n",
        "assert tf.gradients(reference_qvalues, rewards_ph)[0] is not None, \"make sure you used rewards\"\n",
        "assert tf.gradients(reference_qvalues, next_obs_ph)[0] is not None, \"make sure you used next states\"\n",
        "assert tf.gradients(reference_qvalues, obs_ph)[0] is None, \"reference qvalues shouldn't depend on current observation!\" # ignore if you're certain it's ok\n",
        "print(\"Splendid!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fciuIgdcpIjI"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": true,
        "id": "i3dHWxGfpIjI"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "mean_rw_history = []\n",
        "td_loss_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "id": "C5bUtOG7pIjI"
      },
      "outputs": [],
      "source": [
        "exp_replay = ReplayBuffer(10**5)\n",
        "play_and_record(agent, env, exp_replay, n_steps=10000)\n",
        "\n",
        "def sample_batch(exp_replay, batch_size):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
        "    return {\n",
        "        obs_ph:obs_batch, actions_ph:act_batch, rewards_ph:reward_batch, \n",
        "        next_obs_ph:next_obs_batch, is_done_ph:is_done_batch\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "collapsed": true,
        "id": "2XFY4yjcpIjI",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 5200/10000 [15:00<13:51,  5.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "buffer size = 62010, epsilon = 0.28978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Studia\\SEM 3\\GUzW\\notebooks\\Lab. 8.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Studia/SEM%203/GUzW/notebooks/Lab.%208.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(mean_rw_history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Studia/SEM%203/GUzW/notebooks/Lab.%208.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m plt\u001b[39m.\u001b[39mgrid()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Studia/SEM%203/GUzW/notebooks/Lab.%208.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plt\u001b[39m.\u001b[39;49mshow()\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py:421\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mexplicitly there.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m _get_backend_mod()\u001b[39m.\u001b[39mshow(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:41\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 41\u001b[0m         display(\n\u001b[0;32m     42\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[0;32m     43\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[0;32m     44\u001b[0m         )\n\u001b[0;32m     45\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\formatters.py:178\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    176\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\formatters.py:222\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    223\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    340\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    341\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 151\u001b[0m fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mprint_figure(bytes_io, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    152\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backend_bases.py:2314\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[0;32m   2309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[0;32m   2310\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m   2311\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2313\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2314\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m   2316\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2317\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\figure.py:3071\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3068\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3070\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3071\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3072\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3074\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3075\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py:3071\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3068\u001b[0m     \u001b[39mfor\u001b[39;00m spine \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspines\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m   3069\u001b[0m         artists\u001b[39m.\u001b[39mremove(spine)\n\u001b[1;32m-> 3071\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_title_position(renderer)\n\u001b[0;32m   3073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxison:\n\u001b[0;32m   3074\u001b[0m     \u001b[39mfor\u001b[39;00m _axis \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis_map\u001b[39m.\u001b[39mvalues():\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py:3015\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3013\u001b[0m top \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(top, bb\u001b[39m.\u001b[39mymax)\n\u001b[0;32m   3014\u001b[0m \u001b[39mif\u001b[39;00m title\u001b[39m.\u001b[39mget_text():\n\u001b[1;32m-> 3015\u001b[0m     ax\u001b[39m.\u001b[39;49myaxis\u001b[39m.\u001b[39;49mget_tightbbox(renderer)  \u001b[39m# update offsetText\u001b[39;00m\n\u001b[0;32m   3016\u001b[0m     \u001b[39mif\u001b[39;00m ax\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39moffsetText\u001b[39m.\u001b[39mget_text():\n\u001b[0;32m   3017\u001b[0m         bb \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39moffsetText\u001b[39m.\u001b[39mget_tightbbox(renderer)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:1253\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[1;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     renderer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39m_get_renderer()\n\u001b[0;32m   1251\u001b[0m ticks_to_draw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_ticks()\n\u001b[1;32m-> 1253\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_label_position(renderer)\n\u001b[0;32m   1255\u001b[0m \u001b[39m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m tlb1, tlb2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:2506\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2502\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   2504\u001b[0m \u001b[39m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[0;32m   2505\u001b[0m \u001b[39m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[1;32m-> 2506\u001b[0m bboxes, bboxes2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_tick_boxes_siblings(renderer\u001b[39m=\u001b[39;49mrenderer)\n\u001b[0;32m   2507\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mget_position()\n\u001b[0;32m   2508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_position \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:2055\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2053\u001b[0m \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m grouper\u001b[39m.\u001b[39mget_siblings(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes):\n\u001b[0;32m   2054\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(ax, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2055\u001b[0m     ticks_to_draw \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49m_update_ticks()\n\u001b[0;32m   2056\u001b[0m     tlb, tlb2 \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[0;32m   2057\u001b[0m     bboxes\u001b[39m.\u001b[39mextend(tlb)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:1198\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     tick\u001b[39m.\u001b[39mset_label1(label)\n\u001b[0;32m   1197\u001b[0m     tick\u001b[39m.\u001b[39mset_label2(label)\n\u001b[1;32m-> 1198\u001b[0m minor_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_minorticklocs()\n\u001b[0;32m   1199\u001b[0m minor_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminor\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39mformat_ticks(minor_locs)\n\u001b[0;32m   1200\u001b[0m minor_ticks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_minor_ticks(\u001b[39mlen\u001b[39m(minor_locs))\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:1423\u001b[0m, in \u001b[0;36mAxis.get_minorticklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1421\u001b[0m minor_locs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminor\u001b[39m.\u001b[39mlocator())\n\u001b[0;32m   1422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_overlapping_locs:\n\u001b[1;32m-> 1423\u001b[0m     major_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmajor\u001b[39m.\u001b[39;49mlocator()\n\u001b[0;32m   1424\u001b[0m     transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale\u001b[39m.\u001b[39mget_transform()\n\u001b[0;32m   1425\u001b[0m     tr_minor_locs \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(minor_locs)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\ticker.py:2143\u001b[0m, in \u001b[0;36mMaxNLocator.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   2142\u001b[0m     vmin, vmax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis\u001b[39m.\u001b[39mget_view_interval()\n\u001b[1;32m-> 2143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtick_values(vmin, vmax)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\ticker.py:2151\u001b[0m, in \u001b[0;36mMaxNLocator.tick_values\u001b[1;34m(self, vmin, vmax)\u001b[0m\n\u001b[0;32m   2148\u001b[0m     vmin \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mvmax\n\u001b[0;32m   2149\u001b[0m vmin, vmax \u001b[39m=\u001b[39m mtransforms\u001b[39m.\u001b[39mnonsingular(\n\u001b[0;32m   2150\u001b[0m     vmin, vmax, expander\u001b[39m=\u001b[39m\u001b[39m1e-13\u001b[39m, tiny\u001b[39m=\u001b[39m\u001b[39m1e-14\u001b[39m)\n\u001b[1;32m-> 2151\u001b[0m locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_ticks(vmin, vmax)\n\u001b[0;32m   2153\u001b[0m prune \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prune\n\u001b[0;32m   2154\u001b[0m \u001b[39mif\u001b[39;00m prune \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlower\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\ticker.py:2090\u001b[0m, in \u001b[0;36mMaxNLocator._raw_ticks\u001b[1;34m(self, vmin, vmax)\u001b[0m\n\u001b[0;32m   2088\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nbins \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   2089\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2090\u001b[0m         nbins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis\u001b[39m.\u001b[39;49mget_tick_space(),\n\u001b[0;32m   2091\u001b[0m                         \u001b[39mmax\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_min_n_ticks \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m9\u001b[39m)\n\u001b[0;32m   2092\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2093\u001b[0m         nbins \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axis.py:2675\u001b[0m, in \u001b[0;36mYAxis.get_tick_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2674\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tick_space\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 2675\u001b[0m     ends \u001b[39m=\u001b[39m mtransforms\u001b[39m.\u001b[39;49mBbox\u001b[39m.\u001b[39;49munit()\u001b[39m.\u001b[39;49mtransformed(\n\u001b[0;32m   2676\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes\u001b[39m.\u001b[39;49mtransAxes \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi_scale_trans)\n\u001b[0;32m   2677\u001b[0m     length \u001b[39m=\u001b[39m ends\u001b[39m.\u001b[39mheight \u001b[39m*\u001b[39m \u001b[39m72\u001b[39m\n\u001b[0;32m   2678\u001b[0m     \u001b[39m# Having a spacing of at least 2 just looks good.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\transforms.py:479\u001b[0m, in \u001b[0;36mBboxBase.transformed\u001b[1;34m(self, transform)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39mConstruct a `Bbox` by statically transforming this one by *transform*.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m pts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_points()\n\u001b[1;32m--> 479\u001b[0m ll, ul, lr \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(np\u001b[39m.\u001b[39;49marray(\n\u001b[0;32m    480\u001b[0m     [pts[\u001b[39m0\u001b[39;49m], [pts[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m], pts[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]], [pts[\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m], pts[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m]]]))\n\u001b[0;32m    481\u001b[0m \u001b[39mreturn\u001b[39;00m Bbox([ll, [lr[\u001b[39m0\u001b[39m], ul[\u001b[39m1\u001b[39m]]])\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\transforms.py:1490\u001b[0m, in \u001b[0;36mTransform.transform\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   1487\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dims))\n\u001b[0;32m   1489\u001b[0m \u001b[39m# Transform the values\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_affine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_non_affine(values))\n\u001b[0;32m   1492\u001b[0m \u001b[39m# Convert the result back to the shape of the input values.\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m ndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\transforms.py:2420\u001b[0m, in \u001b[0;36mCompositeGenericTransform.transform_affine\u001b[1;34m(self, points)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_affine\u001b[39m(\u001b[39mself\u001b[39m, points):\n\u001b[0;32m   2419\u001b[0m     \u001b[39m# docstring inherited\u001b[39;00m\n\u001b[1;32m-> 2420\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_affine()\u001b[39m.\u001b[39mtransform(points)\n",
            "File \u001b[1;32mc:\\Users\\Bartek\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\transforms.py:2446\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_b\u001b[39m.\u001b[39mget_affine()\n\u001b[0;32m   2445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2446\u001b[0m     \u001b[39mreturn\u001b[39;00m Affine2D(np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_b\u001b[39m.\u001b[39;49mget_affine()\u001b[39m.\u001b[39;49mget_matrix(),\n\u001b[0;32m   2447\u001b[0m                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_a\u001b[39m.\u001b[39;49mget_affine()\u001b[39m.\u001b[39;49mget_matrix()))\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in trange(100000):\n",
        "    \n",
        "    # play\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    \n",
        "    # train\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    \n",
        "    # adjust agent parameters\n",
        "    if i % 200 == 0:\n",
        "        load_weigths_into_target_network(agent, target_network)\n",
        "        agent.epsilon = max(agent.epsilon * 0.98, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        \n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9uzRrj2-pIjJ"
      },
      "outputs": [],
      "source": [
        "assert np.mean(mean_rw_history[-10:]) > 10.\n",
        "print(\"That's good enough for tutorial.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKaeoeGxpIjJ"
      },
      "source": [
        "__ How to interpret plots: __\n",
        "\n",
        "\n",
        "This aint no supervised learning so don't expect anything to improve monotonously. \n",
        "* __ TD loss __ is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
        "* __ mean reward__ is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
        " * In basic q-learning implementation it takes 5-10k steps to \"warm up\" agent before it starts to get better.\n",
        "* __ buffer size__ - this one is simple. It should go up and cap at max size.\n",
        "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - __ it means you need to increase epsilon__. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
        "\n",
        "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
        "\n",
        "__Training will take time.__ A lot of it actually. An optimistic estimate is to say it's gonna start winning (average reward > 10) after 10k steps. \n",
        "\n",
        "But hey, look on the bright side of things:\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/training.png)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
